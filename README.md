<p align="center">
  <img src="https://img.shields.io/badge/Privacy-First-green?style=for-the-badge" alt="Privacy First"/>
  <img src="https://img.shields.io/badge/RAG-Hybrid%20Retrieval-blue?style=for-the-badge" alt="Hybrid RAG"/>
  <img src="https://img.shields.io/badge/Embedding-Qwen3--8B-purple?style=for-the-badge" alt="Qwen3 Embedding"/>
</p>

<h1 align="center">ğŸ“š ERR â€” Ephemeral RAG Reader</h1>

<p align="center">
  <strong>Privacy-first document Q&A with state-of-the-art hybrid retrieval</strong>
</p>

<p align="center">
  Upload a document â†’ Ask questions â†’ Get cited answers<br/>
  <em>Everything stays in memory. Nothing is stored. Your data is yours.</em>
</p>

---

## âœ¨ Features

| Feature | Description |
|---------|-------------|
| ğŸ”’ **Privacy-First** | All processing happens in-memory with automatic TTL cleanup. No database, no persistence. |
| ğŸ¯ **Hybrid Retrieval** | Combines FAISS vector search + BM25 keyword matching for superior recall |
| ğŸ§  **Qwen3-Embedding-8B** | MTEB #1 multilingual embedding model with MRL (Matryoshka) support |
| ğŸ“ **Strict Citations** | Every answer includes `[1][2]` style citations linking to source passages |
| âš¡ **Fast Mode** | Toggle between accuracy (full pipeline) and speed (MRL 1024-dim search) |
| ğŸŒ **100+ Languages** | Full multilingual support inherited from Qwen3 |
| ğŸ“„ **Multiple Formats** | Supports `.txt`, `.md`, `.docx`, `.epub`, `.mobi` |

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Frontend (Next.js 16)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚  Upload  â”‚  â”‚   Chat   â”‚  â”‚   Logs   â”‚  â”‚ Citation Viewer  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚             â”‚             â”‚                 â”‚
        â–¼             â–¼             â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Backend (FastAPI)                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚                    Session Store (In-Memory)                 â”‚â”‚
â”‚  â”‚  â€¢ Chunks + Embeddings    â€¢ FAISS Index    â€¢ BM25 Index     â”‚â”‚
â”‚  â”‚  â€¢ Chat History           â€¢ TTL Cleanup                      â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                              â”‚                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              Retrieval Pipeline                            â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚  HyDE   â”‚â†’ â”‚ Multi-  â”‚â†’ â”‚ Hybrid  â”‚â†’ â”‚ LLM Rerank   â”‚  â”‚  â”‚
â”‚  â”‚  â”‚         â”‚  â”‚ Query   â”‚  â”‚ Search  â”‚  â”‚ (optional)   â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   OpenRouter API    â”‚
                    â”‚  â€¢ Embeddings       â”‚
                    â”‚  â€¢ Chat Completion  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.11+**
- **Node.js 20.9+** (Next.js 16 requirement)
- **OpenRouter API Key** â€” [Get one here](https://openrouter.ai/keys)

### Option 1: Docker (Recommended)

```bash
# 1. Clone the repo
git clone https://github.com/yourusername/book-rag.git
cd book-rag

# 2. Configure your API key
cp backend/.env.example backend/.env
# Edit backend/.env and set OPENROUTER_API_KEY=your_key_here

# 3. Start everything
docker compose up --build

# 4. Open http://localhost:3000
```

### Option 2: Manual Setup

**Backend:**
```bash
cd backend
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate
pip install -r requirements.txt

cp .env.example .env
# Edit .env and set OPENROUTER_API_KEY

uvicorn backend.app.main:app --reload --port 8000
```

**Frontend:**
```bash
cd frontend
npm install
cp .env.example .env.local
npm run dev
```

Open [http://localhost:3000](http://localhost:3000) ğŸ‰

## ğŸ“– How It Works

### 1. Document Ingestion

When you upload a document:

1. **Parse** â€” Extract text blocks from `.txt`, `.md`, `.docx`, `.epub`, or `.mobi`
2. **Chunk** â€” Split into ~512-token chunks with 50-token overlap for context continuity
3. **Embed** â€” Generate 4096-dim vectors using Qwen3-Embedding-8B via OpenRouter
4. **Index** â€” Build FAISS (vector) + BM25 (keyword) indexes in memory

### 2. Query Processing (Normal Mode)

```
User Query
    â”‚
    â”œâ”€â†’ Language Alignment (translate if needed)
    â”‚
    â”œâ”€â†’ Query Expansion
    â”‚   â”œâ”€â†’ Generate 6 query variants (Multi-Query)
    â”‚   â””â”€â†’ Generate hypothetical passage (HyDE)
    â”‚
    â”œâ”€â†’ Embed all variants (instruction-aware)
    â”‚
    â”œâ”€â†’ Hybrid Search (per variant)
    â”‚   â”œâ”€â†’ FAISS: top-50 by vector similarity
    â”‚   â””â”€â†’ BM25: top-50 by keyword match
    â”‚
    â”œâ”€â†’ RRF Fusion (combine all rankings)
    â”‚
    â”œâ”€â†’ LLM Rerank (judge relevance)
    â”‚
    â”œâ”€â†’ Re-pack (reverse order for attention)
    â”‚
    â””â”€â†’ Generate Answer (with citations)
```

### 3. Fast Mode

Toggle "Fast Mode" for quicker responses:

| Feature | Normal Mode | Fast Mode |
|---------|-------------|-----------|
| Search Dimension | 4096 | 1024 (MRL) |
| Query Expansion | âœ… Multi-Query + HyDE | âŒ |
| LLM Rerank | âœ… | âŒ |
| Re-packing | âœ… Reverse | âŒ |
| Embedding Aggregation | Weighted | Simple Mean |

## âš™ï¸ Configuration

All settings are in `backend/.env`. Key options:

```bash
# Models
# Simple tasks (translation, HyDE, QA) - use lighter/faster model
OPENROUTER_CHAT_MODEL_SIMPLE=google/gemini-2.5-flash-lite-preview-09-2025
# Complex tasks (multi-query expansion, LLM rerank) - use more capable model
OPENROUTER_CHAT_MODEL_COMPLEX=google/gemini-2.5-flash-preview-09-2025
OPENROUTER_EMBEDDING_MODEL=qwen/qwen3-embedding-8b

# Retrieval Pipeline
ERR_QUERY_FUSION_ENABLED=true      # Multi-query expansion
ERR_HYDE_ENABLED=true              # Hypothetical document embedding
ERR_LLM_RERANK_ENABLED=true        # LLM-based reranking
ERR_REPACK_STRATEGY=reverse        # Put best chunks near query

# Performance
ERR_EMBEDDING_DIM_FAST_MODE=1024   # MRL dimension for fast mode
ERR_SESSION_TTL_SECONDS=1800       # Session timeout (30 min)
```

> **Note:** `OPENROUTER_CHAT_MODEL` is deprecated and no longer used. Use `OPENROUTER_CHAT_MODEL_SIMPLE` and `OPENROUTER_CHAT_MODEL_COMPLEX` instead.

See [`backend/.env.example`](backend/.env.example) for all options.

## ğŸ§ª Testing

```bash
# Backend unit tests
cd backend
python -m unittest discover -s tests -p "test_*.py"

# Frontend lint
cd frontend
npm run lint
```

## ğŸ“ Project Structure

```
book-rag/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py              # FastAPI app, routes, orchestration
â”‚   â”‚   â”œâ”€â”€ config.py            # Settings & env loading
â”‚   â”‚   â”œâ”€â”€ openrouter_client.py # OpenRouter API wrapper
â”‚   â”‚   â”œâ”€â”€ session_store.py     # In-memory session management
â”‚   â”‚   â”œâ”€â”€ guardrails.py        # Citation enforcement
â”‚   â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â”‚   â”œâ”€â”€ file_parser.py   # Document parsing
â”‚   â”‚   â”‚   â””â”€â”€ chunker.py       # Text chunking with overlap
â”‚   â”‚   â””â”€â”€ retrieval/
â”‚   â”‚       â””â”€â”€ hybrid_retriever.py  # FAISS + BM25 hybrid search
â”‚   â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ .env.example
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ app/                     # Next.js App Router
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ UploadPanel.tsx      # File upload UI
â”‚   â”‚   â”œâ”€â”€ ChatPanel.tsx        # Chat interface
â”‚   â”‚   â””â”€â”€ TerminalWindow.tsx   # Ingestion logs
â”‚   â””â”€â”€ lib/
â”‚       â”œâ”€â”€ store.ts             # Zustand state
â”‚       â””â”€â”€ types.ts             # TypeScript types
â”œâ”€â”€ docker-compose.yml           # Dev environment
â””â”€â”€ docker-compose.prod.yml      # Production-like setup
```

## ğŸ”¬ Technical Highlights

### Qwen3-Embedding-8B

We use the **#1 ranked model on MTEB Multilingual** (score: 70.58 as of June 2025):

- **4096 dimensions** (full) or **1024 dimensions** (MRL fast mode)
- **Instruction-aware** â€” queries use task-specific prompts for +1-5% accuracy
- **100+ languages** including code
- **32K context** for long documents

### Hybrid Retrieval

Combines the best of both worlds:

- **FAISS (Dense)** â€” Semantic similarity via cosine distance
- **BM25 (Sparse)** â€” Keyword matching for exact terms
- **RRF Fusion** â€” Reciprocal Rank Fusion to merge rankings

### Privacy by Design

- **No database** â€” Everything lives in memory
- **TTL cleanup** â€” Sessions auto-expire after 30 minutes
- **No logging of content** â€” Document text never hits disk

## ğŸ¤ Contributing

1. Fork the repo
2. Create a feature branch (`git checkout -b feat/amazing-feature`)
3. Commit with [Conventional Commits](https://www.conventionalcommits.org/) (`git commit -m 'feat: add amazing feature'`)
4. Push and open a PR

## ğŸ“„ License

[Apache 2.0](LICENSE)

---

<p align="center">
  Built with â¤ï¸ using FastAPI, Next.js, and Qwen3-Embedding
</p>
